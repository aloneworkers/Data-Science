{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn 10 | Bag of Words & Stop Word Filtering | Text Processing | Belajar Machine Learning\n",
    "\n",
    "## Bag of Words model sebagai representasi text\n",
    "\n",
    "Bag of Words menyederhanakan representasi text sebagai sekumpulan kata serta mengabaikan grammar dan posisi tiap kata pada kalimat. Text akan dikonversi menjadi lowercase dan tanda baca akan diabaikan.\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "Materi Pembelajaran dikutip dari sumber: https://www.youtube.com/watch?v=U30sF4m0bd0\n",
    "\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',\n",
    "    'Linux distributions include the Linux kernel.',\n",
    "    'Linux is one of the most prominent open-source software.'\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dataset untuk kasus kali ini berupa sekumpulan kalimat pendek.\n",
    "+ Dataset ini juga sering dikenal dengan istilah corpus.\n",
    "+ Corpus terdiri dari 3 kalimat pendek, yaitu: \n",
    "\n",
    "    1. Linux has been around since the mid-1990s.\n",
    "    2. Linux distributions include the Linux kernel.\n",
    "    3. Linux is one of the most prominent open-source software.\n",
    "\n",
    "\n",
    "+ 3 kalimat tersebut akan ditampung kedalam sebuah list yang kemudian di assign kedalam suatu variable bernama corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words model dengan CountVectorizer\n",
    "\n",
    "Bag of Words model dapat diterapkan dengan memanfatkan CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menerapkan Bag of Wodrs model dengan memanfaatkan CountVectorizer:\n",
    "\n",
    "+ Melakukan import modul terlebih dahulu dengan memanggil \"from sklearn.feature_extraction.text import CountVectorizer\".\n",
    "+ Membentuk objek dari class \"CountVectorizer()\" yang ditampung kedalam variable vectorizer.\n",
    "+ Menggunakan objek vectorizer untuk menerapkan method fit_transform terhadap corpus dataset kemudian hasil yang terbentuk akan dikonversikan kedalam suatu array dengan memanggil method \"todense()\" akan mengkonversikan hasil \"fit_transform()\" dari objek vectorizer menjadi suatu array 2 dimensi kemudian ditampung kedalam variable vectorized_X.\n",
    "+ Kemudian tampilkan hasil yang berupa array 2 dimensi dan setiap baris akan merepresentasikan tiap kalimat yang berada dalam corpus.\n",
    "+ Baris pertama merepresentasikan kalimat pertama dalam corpus.\n",
    "+ Baris kedua merepresentasikan kalimat kedua dalam corpus.\n",
    "+ Baris ketiga merepresentasikan kalimat ketiga dalam corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'of',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Ketika memanggil vectorizer.get_feature_names() maka akan mengembalikan sekumpulan kata yang berada dalam bag atau keranjang.\n",
    "+ Teknik yang digunakan Bag of Words.\n",
    "+ Kumpulan kata sudah tidak diurutkan berdasarkan ururtan kalimat melainkan berdasarkan alphabet.\n",
    "+ Semua case menjadi lowercase.\n",
    "+ Setiap kata yang ditampung dalam bag dikenal dengan istilah token.\n",
    "+ Nilai pada array tidak hanya 0 dan 1, tetapi dilayar hanya muncul array dengan nilai 0 dan 1.\n",
    "+ Setiap nilai merepresentasikan jumlah kemunculan token/kata tertentu pada kalimat.\n",
    "+ index ke 0 merepresentasikan \"1990s\"\n",
    "+ index ke 1 merepresentasikan \"around\", dan seterusnya begitu.\n",
    "+ nilai 0 merepresentasikan kalimat yang tidak memiliki kata yang berada dalam bag atau keranjang.\n",
    "+ nilai 1 merepresentasikan kalimat yang memiliki kata yang berada dalam bag atau keranjang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance untuk mengukur kedekatan/jarak antar dokumen (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2: [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3: [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3: [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i], vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}: {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mengukur kedekatan atau kemiripan antar dokumen:\n",
    "\n",
    "+ Melakukan import modul terlebih dahulu dengan memanggil \"from sklearn.metrics.pairwise import euclidean_distances\".\n",
    "+ Kemudian akan mengukur jarak antar kalimat, berarti kalimat pertama akan diukur kedekatannya dengan kalimat kedua dan kalimat ketiga.\n",
    "+ Selanjutnya akan mengukur kedekatan/jarak antara kalimat kedua dan kalimat ketiga. \n",
    "\n",
    "Hasilnya:\n",
    "\n",
    "+ Jarak antara dokumen pertama dan dokumen kedua adalah 3.16227766.\n",
    "+ Jarak antara dokumen pertama dan dokumen ketiga adalah 3.74165739\n",
    "+ Jarak antara dokumen kedua dan dokumen ketiga adalah 3.46410162\n",
    "\n",
    "Kesimpulan:\n",
    "\n",
    "+ Dapat dilihat nilai terkecilnya adalah antara dokumen pertama dan dokumen kedua, artinya tingkat kemiripan antara dokumen pertama dan dokumen kedua yang paling tinggi diantara ketiga dokumen yang tersedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Filtering pada text\n",
    "\n",
    "Stop Word Filtering menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an), auxiliary verbs (do, be, will), dan prepositions (on, in, at).\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Stop_word\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Untuk kasus kali ini menggunakan corpus atau dataset yang sama seperti yang digunakan sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus terdiri dari 3 kalimat, yaitu:\n",
    "\n",
    "    1. Linux has been around since the mid-1990s.\n",
    "    2. Linux distributions include the Linux kernel.\n",
    "    3. Linux is one of the most prominent open-source software.\n",
    "\n",
    "Dapat diperhatikan pada ketiga kalimat tersebut, terdapat beberapa Stop Word slaha satunya adalah the, has, been, is, of dan itu adalah Stop Word yang akan diabaikan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Filtering dengan CountVectorizer\n",
    "\n",
    "Stop Word Filtering juga dapat diterapkan dengan memanfatkan CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memanfaatkan Stop Filtering untuk mengeluarkan Stop Word dari corpus yang dimiliki.\n",
    "Pada SKLearn Stop Word Filtering juga dapat diterapkan dengan memanfaatkan modul CountVectorizer, berikut langkah-langkahnya: \n",
    "\n",
    "+ Melakukan import modul terlebih dahulu dengan memanggil \"from sklearn.feature_extraction.text import CountVectorizer\".\n",
    "+ Kemudian membentuk objek dari class \"CountVectorizer(stop_words='english'), sewaktu kita membentuk objek CountVectorizer kita juga menyertakan parameter stop_words yang diberi nilai 'english', karena kasusnya untuk melakukan Stop Word Filtering untuk bahasa inggris.\n",
    "+ Lalu selanjutnya objek \"CountVectorizer\" yang terbentuk akan ditampung kedalam variable vectorizer.\n",
    "+ Objek vectorizer akan dipanggil untuk menerapkan fit_transform() terhadap corpus yang dimiliki dan hasil fit_transform() akan dikonversi kedalam array 2 dimensi karena itu dipanggil method .todense().\n",
    "+ Array 2 dimensi yang terbentuk akan ditampung kedalam variable vectorizer_X untuk ditampilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Merupakan kumpulan kata atau token yang sudah difiltering Stop Wordnya.\n",
    "+ Bisa dilihat kita sudah tidak menyertakan Stop Word dalam kumpulan kata atau token."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
